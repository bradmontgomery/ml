{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "# given some content...\n",
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]\n",
    "\n",
    "X = vectorizer.fit_transform(content)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Feature names: {}\".format(feature_names))\n",
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** of above data: Each column gives us a boolean (1 or 0) value letting us know if each word appears in the sentence (from `content`). Sentence 1 (`content[0]`) contains all words but \"problems\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n",
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'this', 'toy']\n",
      "----------------------------------------\n",
      "#samples: 5, #features: 25\n",
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'this', 'toy']\n",
      "----------------------------------------\n",
      "- Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "- Post 1 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "- Post 2 with dist=2.00: Most imaging databases save images permanently.\n",
      "- Post 3 with dist=1.41: Imaging databases store data.\n",
      "- Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=1.41\n",
      "\n",
      "Vectors for what should be similar sentences:\n",
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "posts = [\n",
    "    \"This is a toy post about machine learning. Actually, it contains not much interesting stuff.\",\n",
    "    \"Imaging databases provide storage capabilities.\",\n",
    "    \"Most imaging databases save images permanently.\",\n",
    "    \"Imaging databases store data.\",\n",
    "    \"Imaging databases store data. Imaging databases store data. Imaging databases store data.\",\n",
    "]\n",
    "\n",
    "# Create a training set\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: {}, #features: {}\".format(num_samples, num_features))\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# create a new post\n",
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "# a naive similarity measure (which uses the full ndarray of the new post)\n",
    "import scipy as sp\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    # norm: Euclidean norm (shortest distance)\n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "# Find distances among all posts\n",
    "import sys\n",
    "\n",
    "def find_distances(vectorizer, new_post, posts, dist_func=dist_raw):\n",
    "    X_train = vectorizer.fit_transform(posts)\n",
    "    new_post_vec = vectorizer.transform([new_post])\n",
    "    num_samples, num_features = X_train.shape\n",
    "    \n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"#samples: {}, #features: {}\".format(num_samples, num_features))\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(\"----------------------------------------\")\n",
    "    \n",
    "    best_dist = sys.maxsize\n",
    "    best_i = None\n",
    "\n",
    "    for i in range(0, num_samples):\n",
    "        post = posts[i]\n",
    "        if post == new_post:\n",
    "            continue\n",
    "        post_vec = X_train.getrow(i)\n",
    "        \n",
    "        d = dist_func(post_vec, new_post_vec)\n",
    "\n",
    "        print(\"- Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "\n",
    "    print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n",
    "\n",
    "find_distances(vectorizer, new_post, posts)\n",
    "\n",
    "# explore the vectors for posts 3 & 4 since they all contain the same words\n",
    "print(\"\\nVectors for what should be similar sentences:\")\n",
    "print(X_train.getrow(3).toarray())\n",
    "print(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "#samples: 5, #features: 25\n",
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'this', 'toy']\n",
      "----------------------------------------\n",
      "- Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "- Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "- Post 2 with dist=0.92: Most imaging databases save images permanently.\n",
      "- Post 3 with dist=0.77: Imaging databases store data.\n",
      "- Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "# Normalize the vectors, and try again\n",
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "find_distances(vectorizer, new_post, posts, dist_func=dist_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of our stop words: a, about, above, across, after, afterwards, again, against, all, almost, alone, along, already, also, although, always, am, among, amongst, amoungst\n",
      "#samples: 5, #features: 18\n",
      "['actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'learning', 'machine', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "# using stop words; i.e. removing \"noise\" / useless info\n",
    "# use common english stop words (can also provide a list of specific words)\n",
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "print(\"Some of our stop words: {}\".format(\", \".join(sorted(vectorizer.get_stop_words())[0:20])))\n",
    "\n",
    "# construct a new training set\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: {}, #features: {}\".format(num_samples, num_features))\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphic\n",
      "imag\n",
      "imag\n",
      "imagin\n",
      "imagin\n"
     ]
    }
   ],
   "source": [
    "# Using NLTK for stemming (reducing words to their specific word stem)\n",
    "import nltk.stem as ns\n",
    "s = ns.SnowballStemmer('english')\n",
    "print(s.stem(\"graphics\"))\n",
    "print(s.stem(\"imaging\"))\n",
    "print(s.stem(\"image\"))\n",
    "print(s.stem(\"imagination\"))\n",
    "print(s.stem(\"imagine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "#samples: 5, #features: 17\n",
      "['actual', 'capabl', 'contain', 'data', 'databas', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'provid', 'save', 'storag', 'store', 'stuff', 'toy']\n",
      "----------------------------------------\n",
      "- Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "- Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "- Post 2 with dist=0.63: Most imaging databases save images permanently.\n",
      "- Post 3 with dist=0.77: Imaging databases store data.\n",
      "- Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist=0.63\n"
     ]
    }
   ],
   "source": [
    "# stem our posts before verctorizing\n",
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super().build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "find_distances(vectorizer, new_post, posts, dist_func=dist_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 3529\n",
      "#features: 4712\n",
      "Initialization complete\n",
      "Iteration  0, inertia 5899.560\n",
      "Iteration  1, inertia 3218.298\n",
      "Iteration  2, inertia 3184.333\n",
      "Iteration  3, inertia 3164.867\n",
      "Iteration  4, inertia 3152.004\n",
      "Iteration  5, inertia 3143.111\n",
      "Iteration  6, inertia 3136.256\n",
      "Iteration  7, inertia 3129.325\n",
      "Iteration  8, inertia 3124.567\n",
      "Iteration  9, inertia 3121.900\n",
      "Iteration 10, inertia 3120.210\n",
      "Iteration 11, inertia 3118.627\n",
      "Iteration 12, inertia 3117.363\n",
      "Iteration 13, inertia 3116.811\n",
      "Iteration 14, inertia 3116.588\n",
      "Iteration 15, inertia 3116.417\n",
      "Iteration 16, inertia 3115.760\n",
      "Iteration 17, inertia 3115.374\n",
      "Iteration 18, inertia 3115.155\n",
      "Iteration 19, inertia 3114.949\n",
      "Iteration 20, inertia 3114.515\n",
      "Iteration 21, inertia 3113.937\n",
      "Iteration 22, inertia 3113.720\n",
      "Iteration 23, inertia 3113.548\n",
      "Iteration 24, inertia 3113.475\n",
      "Iteration 25, inertia 3113.447\n",
      "Converged at iteration 25\n",
      "[38 17 47 ..., 41 14 16]\n",
      "(3529,)\n"
     ]
    }
   ],
   "source": [
    "# Finally cluster some stuff\n",
    "# Using data sets from: http://mlcomp.org/datasets/379\n",
    "\n",
    "import sklearn.datasets\n",
    "all_data = sklearn.datasets.fetch_20newsgroups(subset='all')\n",
    "#print(\"Files: {}\".format(len(all_data.filenames)))\n",
    "#print(\"Target Names: {}\".format(\", \".join(all_data.target_names)))\n",
    "\n",
    "# set up some training data\n",
    "groups = [\n",
    "    'comp.graphics',\n",
    "    'comp.os.ms-windows.misc',\n",
    "    'comp.sys.ibm.pc.hardware',\n",
    "    'comp.sys.mac.hardware',\n",
    "    'comp.windows.x',\n",
    "    'sci.space',\n",
    "]\n",
    "train_data = sklearn.datasets.fetch_20newsgroups(subset='train', categories=groups)\n",
    "test_data = sklearn.datasets.fetch_20newsgroups(subset='test', categories=groups)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk.stem\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super().build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))    \n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5, stop_words='english', decode_error='ignore')\n",
    "vectorized = vectorizer.fit_transform(train_data.data)\n",
    "\n",
    "num_samples, num_features = vectorized.shape\n",
    "print(\"#samples: {}\\n#features: {}\".format(num_samples, num_features))\n",
    "\n",
    "\n",
    "# NOW, do the clustering\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=50, init='random', n_init=1, verbose=1, random_state=3)\n",
    "km.fit(vectorized)\n",
    "\n",
    "print(km.labels_)\n",
    "print(km.labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New post label: 7\n",
      "Found 166 similar posts.\n",
      "\n",
      "-- similarity score: 1.0378441731334074 --\n",
      "From: Thomas Dachsel <GERTHD@mvs.sas.com>\n",
      "Subject: BOOT PROBLEM with IDE controller\n",
      "Nntp-Posting-Host: sdcmvs.mvs.sas.com\n",
      "Organization: SAS Institute Inc.\n",
      "Lines: 25\n",
      "\n",
      "Hi,\n",
      "I've got a Multi I/O card (IDE controller + serial/parallel\n",
      "interface) and two floppy drives (5 1/4, 3 1/2) and a\n",
      "Quantum ProDrive 80AT connected to it.\n",
      "I was able to format the hard disk, but I could not boot from\n",
      "it. I can boot from drive A: (which disk drive does not matter)\n",
      "but if I remove the disk from drive A and press the reset switch,\n",
      "the LED of drive A: continues to glow, and the hard disk is\n",
      "not accessed at all.\n",
      "I guess this must be a problem of either the Multi I/o card\n",
      "or floppy disk drive settings (jumper configuration?)\n",
      "Does someone have any hint what could be the reason for it.\n",
      "Please reply by email to GERTHD@MVS.SAS.COM\n",
      "Thanks,\n",
      "Thomas\n",
      "+-------------------------------------------------------------------+\n",
      "| Thomas Dachsel                                                    |\n",
      "| Internet: GERTHD@MVS.SAS.COM                                      |\n",
      "| Fidonet:  Thomas_Dachsel@camel.fido.de (2:247/40)                 |\n",
      "| Subnet:   dachsel@rnivh.rni.sub.org (UUCP in Germany, now active) |\n",
      "| Phone:    +49 6221 4150 (work), +49 6203 12274 (home)             |\n",
      "| Fax:      +49 6221 415101                                         |\n",
      "| Snail:    SAS Institute GmbH, P.O.Box 105307, D-W-6900 Heidelberg |\n",
      "| Tagline:  One bad sector can ruin a whole day...                  |\n",
      "+-------------------------------------------------------------------+\n",
      "\n",
      "--------------------------\n",
      "\n",
      "\n",
      "-- similarity score: 1.1716497460538475 --\n",
      "From: badry@cs.UAlberta.CA (Badry Jason Theodore)\n",
      "Subject: Chaining IDE drives\n",
      "Summary: Trouble with Master/Slave drives\n",
      "Nntp-Posting-Host: cab009.cs.ualberta.ca\n",
      "Organization: University Of Alberta, Edmonton Canada\n",
      "Lines: 16\n",
      "\n",
      "Hi.  I am trying to set up a Conner 3184 and a Quantum 80AT drive.  I have\n",
      "the conner set to the master, and the quantum set to the slave (doesn't work\n",
      "the other way around).  I am able to access both drives if I boot from a \n",
      "floppy, but the drives will not boot themselves.  I am running MSDOS 6, and\n",
      "have the Conner partitioned as Primary Dos, and is formatted with system\n",
      "files.  I have tried all different types of setups, and even changed IDE\n",
      "controller cards.  If I boot from a floppy, everything works great (except\n",
      "the booting part :)).  The system doesn't report an error message or anything,\n",
      "just hangs there.  Does anyone have any suggestions, or has somebody else\n",
      "run into a similar problem?  I was thinking that I might have to update the bios\n",
      "on one of the drives (is this possible?).  Any suggestions/answers would be\n",
      "greatly appreciated.  Please reply to:\n",
      "\n",
      "\tJason Badry\n",
      "\tbadry@cs.ualberta.ca\n",
      "\n",
      "\n",
      "--------------------------\n",
      "\n",
      "\n",
      "-- similarity score: 1.3118266609870635 --\n",
      "From: delman@mipg.upenn.edu (Delman Lee)\n",
      "Subject: Tandberg 3600 + Future Domain TMC-1660 + Seagate ST-21M problem??\n",
      "Distribution: comp\n",
      "Organization: University of Pennsylvania, USA.\n",
      "Lines: 37\n",
      "Nntp-Posting-Host: mipgsun.mipg.upenn.edu\n",
      "\n",
      "I am trying to get my system to work with a Tandberg 3600 + Future\n",
      "Domain TMC-1660 + Seagate ST-21M MFM controller. \n",
      "\n",
      "The system boots up if the Tandberg is disconnected from the system,\n",
      "and of course no SCSI devices found (I have no other SCSI devices).\n",
      "\n",
      "The system boots up if the Seagate MFM controller is removed from the\n",
      "system. The Future Domain card reports finding the Tandberg 3660 on\n",
      "the SCSI bus. The system then of course stops booting because my MFM\n",
      "hard disks can't be found.\n",
      "\n",
      "The system hangs if all three (Tandberg, Future Domain TMC-1660 &\n",
      "Seagate MFM controller) are in the system. \n",
      "\n",
      "Looks like there is some conflict between the Seagate and Future\n",
      "Domain card. But the funny thing is that it only hangs if the Tandberg\n",
      "is connected.\n",
      "\n",
      "I have checked that there are no conflict in BIOS addresses, IRQ & I/O\n",
      "port. Have I missed anything?\n",
      "\n",
      "I am lost here. Any suggestions are most welcomed. Thanks in advance.\n",
      "\n",
      "Delman.\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "______________________________________________________________________\n",
      "\n",
      "  Delman Lee                                 Tel.: +1-215-662-6780\n",
      "  Medical Image Processing Group,            Fax.: +1-215-898-9145\n",
      "  University of Pennsylvania,\n",
      "  4/F Blockley Hall, 418 Service Drive,                         \n",
      "  Philadelphia, PA 19104-6021,\n",
      "  U.S.A..                            Internet: delman@mipg.upenn.edu\n",
      "______________________________________________________________________\n",
      "\n",
      "--------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test out on a new mailling list post.\n",
    "new_post = \"\"\"Disk drive problems. Hi, I have a problem with my hard disk.\n",
    "After 1 year it is working only sporadically now.\n",
    "I tried to format it, but now it doesn't boot any more.\n",
    "Any ideas? Thanks.\n",
    "\"\"\"\n",
    "\n",
    "# vectorize the new post, and predict it's grouping\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0]\n",
    "print(\"New post label: {}\".format(new_post_label))\n",
    "\n",
    "# find similar...\n",
    "similar_indices = (km.labels_ == new_post_label).nonzero()[0]\n",
    "\n",
    "# build a list of similar posts\n",
    "similar = []\n",
    "for i in similar_indices:\n",
    "    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
    "    similar.append((dist, train_data.data[i]))\n",
    "similar = sorted(similar)\n",
    "print(\"Found {} similar posts.\".format(len(similar)))\n",
    "\n",
    "a = similar[0]\n",
    "b = similar[int(len(similar) / 10)]\n",
    "c = similar[int(len(similar) / 2)]\n",
    "\n",
    "fmt = \"\\n-- similarity score: {} --\\n{}\\n--------------------------\\n\"\n",
    "print(fmt.format(a[0], a[1]))\n",
    "print(fmt.format(b[0], b[1]))\n",
    "print(fmt.format(c[0], c[1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
